{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.springboard.com/blog/data-science-interview-questions/\n",
    "\n",
    "How do you deal with sparsity?\n",
    "LASSO and its variants (e.g. LASSO + Boosting) might be interesting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independence\n",
    "\n",
    "Two events A and B are independent $\\Leftrightarrow \\mathcal{P}(A\\cap B) = \\mathcal{P}(A) \\mathcal{P}(B)$\n",
    "\n",
    "#### Conditional\n",
    "\n",
    "For events A and B such that $\\mathcal{P}(B) \\neq 0$, we have:\n",
    "\n",
    "$\\displaystyle \\mathcal{P}(A\\mid B)={\\frac {\\mathcal{P}(A\\cap B)}{P(B)}} = \\frac{\\mathcal{P}(B\\mid A)\\mathcal{P}(A)}{\\mathcal{P}(B)}   $\n",
    "\n",
    "#### Law of total probability\n",
    "\n",
    "$\\displaystyle \\mathcal{P}(A)=\\sum _{n}\\mathcal{P}(A\\cap B_{n})$\n",
    "\n",
    "or, alternatively,\n",
    "\n",
    "$\\displaystyle \\mathcal{P}(A)=\\sum _{n}\\mathcal{P}(A\\mid B_{n})\\mathcal{P}(B_{n})$\n",
    "\n",
    "#### Inclusion–exclusion principle\n",
    "\n",
    "$\\displaystyle \\left|\\bigcup _{i=1}^{n}A_{i}\\right|=\\sum _{\\emptyset \\neq J\\subseteq \\{1,\\ldots ,n\\}}(-1)^{|J|+1}\\left|\\bigcap _{j\\in J}A_{j}\\right|$\n",
    "\n",
    "example: $|A\\cup B\\cup C|=|A|+|B|+|C|-|A\\cap B|-|A\\cap C|-|B\\cap C|+|A\\cap B\\cap C|$\n",
    "\n",
    "#### Union, Intersection and Complement\n",
    "\n",
    "$\\mathcal{P}(A\\cup B) = \\mathcal{P}(A) + \\mathcal{P}(B) - \\mathcal{P}(A\\cap B)$\n",
    "\n",
    "$\\displaystyle ({A\\cup B})^{C}=A^{C}\\cap B^{C}$\n",
    "\n",
    "$\\displaystyle ({A\\cap B})^{C}=A^{C}\\cup B^{C}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "$E$ a set of $n$ elements, $Card(\\mathcal{P}(E)) = 2^n$\n",
    "\n",
    "| Sampling  | Order matters | Order not matters |\n",
    "| ------------- | ------------- | ---------- |\n",
    "| With replacement  | $\\displaystyle n^k$  | $\\displaystyle \\binom{n+k-1}{k}$     |\n",
    "| Without replacement  | $\\displaystyle \\frac{n!}{(n-k)!}$  | $\\displaystyle \\binom{n}{k}$   |\n",
    "\n",
    "In particular, the number of **permutation** of a set of $n$ elements is a sample where order matters without replacement => $n!$\n",
    "\n",
    "\n",
    "Simple examples:\n",
    "- choose 4 balls in a 10-balls numbered box successively with replacement => $10^4$\n",
    "- choose 4 balls in a 10-balls numbered box successively without replacement => $\\displaystyle  \\frac{10!}{6!}$\n",
    "- choose 4 balls in a 10-balls box simultaneously => $\\displaystyle \\binom{10}{4}$\n",
    "\n",
    "\n",
    "Exercises:\n",
    "- Loto: pick 7 numbers out of 49\n",
    "- probability that at least 2 students have the same date of birth in a class of 30"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usual Laws\n",
    "\n",
    "#### discrets\n",
    "\n",
    "<img src=\"pictures/law_discret.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "#### continus\n",
    "\n",
    "<img src=\"pictures/law_continus.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain teaser \n",
    "\n",
    "- 2 Heads 1 tail, split the pot 1/4\n",
    "- two children, one is a girl, proba the second is a girl ?\n",
    "- revolver\n",
    "- palantir\n",
    "- google\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO :\n",
    "- Monte carlo \n",
    "- Markov chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "The variance of a random variable ${\\displaystyle X}$ is the expected value of the squared deviation from the mean of ${\\displaystyle X}$:\n",
    "\n",
    "${\\displaystyle \\operatorname {Var} (X)=\\operatorname {E} \\left[(X-\\operatorname {E} [X] )^{2}\\right]} = \\operatorname {E} [X^{2}]-\\operatorname {E} [X]^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var([0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance\n",
    "\n",
    "In probability theory and statistics, covariance is a measure of the joint variability of two random variables.\n",
    "\n",
    "If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, (i.e., the variables tend to show similar behavior), the covariance is positive.\n",
    "\n",
    "In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, (i.e., the variables tend to show opposite behavior), the covariance is negative. \n",
    "\n",
    "The sign of the covariance therefore shows the tendency in the linear relationship between the variables. \n",
    "\n",
    "The magnitude of the covariance is not easy to interpret because it is not normalized and hence depends on the magnitudes of the variables. \n",
    "\n",
    "The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation.\n",
    "\n",
    "${\\displaystyle \\operatorname {cov} (X,Y)=\\operatorname {E} {{\\big [}(X-\\operatorname {E} [X])(Y-\\operatorname {E} [Y]){\\big ]}}} = \\operatorname {E} \\left[XY\\right]-\\operatorname {E} \\left[X\\right]\\operatorname {E} \\left[Y\\right]\t$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov([1,1,1],[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.],\n",
       "       [-1.,  1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov([2,1,0],[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov([0,1,2],[2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.5       ],\n",
       "       [1.5       , 2.33333333]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov([0,1,2],[2,3,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation coefficient\n",
    "\n",
    "Pearson correlation coefficient is a measure of the linear correlation between two variables X and Y. According to the Cauchy–Schwarz inequality it has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation.\n",
    "\n",
    "${\\displaystyle \\rho _{X,Y}={\\frac {\\operatorname {cov} (X,Y)}{\\sigma _{X}\\sigma _{Y}}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan],\n",
       "       [nan,  1.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef([1,1,1],[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.],\n",
       "       [-1.,  1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef([2,1,0],[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef([0,1,2],[2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.88968025],\n",
       "       [0.88968025, 1.        ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef([0,1,2],[2,3,20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-correlation\n",
    "\n",
    "Cross-correlation is somewhat a generalization of the correlation measure as it takes into account the lag of one signal relative to the other. If lag == 0, then correlation = cross-correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman's rank correlation coefficient\n",
    "\n",
    "Spearman's rank correlation coefficient is a nonparametric measure of rank correlation (statistical dependence between the rankings of two variables). It assesses how well the relationship between two variables can be described using a monotonic function.\n",
    "\n",
    "For a sample of size n, the n raw scores ${\\displaystyle X_{i},Y_{i}}$ are converted to ranks ${\\displaystyle \\operatorname {rg} X_{i},\\operatorname {rg} Y_{i}}$, and $r_{s}$ is computed from:\n",
    "\n",
    "${\\displaystyle r_{s}=\\rho _{\\operatorname {rg} _{X},\\operatorname {rg} _{Y}}={\\frac {\\operatorname {cov} (\\operatorname {rg} _{X},\\operatorname {rg} _{Y})}{\\sigma _{\\operatorname {rg} _{X}}\\sigma _{\\operatorname {rg} _{Y}}}}}$\n",
    "\n",
    "where:\n",
    "- $\\rho $ denotes the usual Pearson correlation coefficient, but applied to the rank variables.\n",
    "- ${\\displaystyle \\operatorname {cov} (\\operatorname {rg} _{X},\\operatorname {rg} _{Y})}$ is the covariance of the rank variables.\n",
    "- ${\\displaystyle \\sigma _{\\operatorname {rg} _{X}}}{\\displaystyle \\sigma _{\\operatorname {rg} _{X}}}$ and ${\\displaystyle \\sigma}{\\displaystyle \\sigma }$ are the standard deviations of the rank variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson : 0.8664578754912147\n",
      "SpearmanrResult(correlation=1.0, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "x = [1,2,3]\n",
    "y = [1,1000,1000000]\n",
    "print(\"pearson : {}\".format(pearson(x,y)))\n",
    "print(st.spearmanr(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson : 0.25203203390387025\n",
      "SpearmanrResult(correlation=0.9999999999999999, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "x = list(range(100))\n",
    "y = np.exp(x)                        \n",
    "print(\"pearson : {}\".format(pearson(x,y)))\n",
    "print(st.spearmanr(x,y))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLT\n",
    "\n",
    "The central limit theorem establishes that, when identical independent random variables are added, their normalized sum tends toward a normal distribution (informally a \"bell curve\") even if the original variables themselves are not normally distributed. \n",
    "\n",
    "Let ${X_1, …, X_n}$ be a random sample of size n—that is, a sequence of independent and identically distributed (i.i.d.) random variables drawn from a distribution of expected value given by µ and finite variance given by $\\sigma^2$. \n",
    "\n",
    "Suppose we are interested in the sample average ${\\displaystyle S_{n}={\\frac {X_{1}+\\cdots +X_{n}}{n}}}$\n",
    "of these random variables. \n",
    "\n",
    "By the law of large numbers, the sample averages converge in probability and almost surely to the expected value $\\mu$ as n → ∞. \n",
    "\n",
    "The classical central limit theorem describes the size and the distributional form of the stochastic fluctuations around the deterministic number $\\mu$ during this convergence: \n",
    "\n",
    "${\\displaystyle \\frac {\\left(S_{n}-\\mu \\right)} {\\sigma/\\sqrt {n} }\\ {\\xrightarrow {\\mathcal {L}}}\\ {\\mathcal {N}}(0 ,1).}$\n",
    "\n",
    "Use case: Suppose that we are interested in estimating the average height among all people. Collecting data for every person in the world is impossible. While we can’t obtain a height measurement from everyone in the population, we can still sample some people. The question now becomes, what can we say about the average height of the entire population given a single sample. The Central Limit Theorem addresses this question exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling distribution\n",
    "\n",
    "Given a population has a mean $\\mu$ and a standard deviation $\\sigma$, a sample of it has:\n",
    "\n",
    "$\\operatorname {E} \\left[\\bar  x\\right] \\sim \\mu$\n",
    "\n",
    "$\\displaystyle \\sigma _{\\bar  x} \\sim {\\frac  {\\sigma }{{\\sqrt  {n}}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-score\n",
    "\n",
    "$\\displaystyle \\mathcal{Z} = \\frac {\\operatorname {E} \\left[\\bar  x\\right] - \\mu}{\\sigma _\\bar  x} $\n",
    "\n",
    "$\\displaystyle \\mathcal{Z}$ can be expressed as the *number of standard deviation below the mean*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = [-3,1,2,3,6,9]\n",
    "Z = (np.mean(l) - 0) / (np.std(l)/math.sqrt(len(l)))\n",
    "\n",
    "mu = 0\n",
    "variance = 1\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "plt.plot(x, st.norm.pdf(x, mu, sigma))\n",
    "plt.axvline(Z,c=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence interval\n",
    "\n",
    "$\\displaystyle CI = \\operatorname {E} \\left[\\bar x \\right] \\pm z^* \\sigma _{\\bar  x}  $\n",
    "\n",
    "Where:\n",
    "- $z^*$ is the value corresponding to the $\\alpha$ (confidence) quantile from the normal distribution ${\\mathcal {N}}(0 ,1)$\n",
    "- $\\displaystyle z^* \\sigma _{\\bar  x}$ can be expressed as the *margin of error*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.959963984540054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_2_tails = .95\n",
    "quantile_1_tail = (1+quantile_2_tails)/2\n",
    "z = st.norm.ppf(quantile_1_tail)\n",
    "print(z)\n",
    "st.norm.cdf(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = [-3,1,2,3,6,9]\n",
    "\n",
    "mean = np.mean(l)\n",
    "me = z*np.std(l)/math.sqrt(len(l))\n",
    "left = mean-me\n",
    "right = mean+me\n",
    "\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(mean - 3*me, mean + 3*me, 100)\n",
    "plt.plot(x, st.norm.pdf(x, mean, 1))\n",
    "plt.axvline(left,c=\"r\")\n",
    "plt.axvline(right,c=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p-value\n",
    "\n",
    "In statistical hypothesis testing, the p-value or probability value is the probability of obtaining test results at least as extreme as the results actually observed during the test, assuming that the null hypothesis is correct.\n",
    "\n",
    "If the probability of obtaining a result as extreme as the one obtained, supposing that the null hypothesis were true, is lower than a pre-specified cut-off probability (for example, 5%), then the result is said to be statistically significant and the null hypothesis is rejected.\n",
    "\n",
    "$\\displaystyle p = \\mathcal{P} \\left(\\mathcal{Z}>z\\right) < \\alpha \n",
    "\\Leftrightarrow \\mathcal{Z} > z^*\n",
    "\\Longleftrightarrow \\frac {\\operatorname {E} \\left[\\bar  x\\right] - \\mu}{\\sigma _\\bar  x} > z^* $\n",
    "\n",
    "Where:\n",
    "- $z^*$ is the value corresponding to the $\\alpha$ (confidence) quantile from the normal distribution ${\\mathcal {N}}(0 ,1)$\n",
    "\n",
    "##### Example:\n",
    "\n",
    "A principal at a certain school claims that the students in his school are above average intelligence. A random sample of thirty students IQ scores have a mean score of 112. Is there sufficient evidence to support the principal’s claim? The mean population IQ is 100 with a standard deviation of 15.\n",
    "\n",
    "- H0: “the student of his school are not on average more intelligent”\n",
    "- H1: “the student of his school are on average more intelligent”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = 100\n",
    "mean_x_bar = 112\n",
    "SE = 15/math.sqrt(30)\n",
    "\n",
    "Z = (mean_x_bar-mu)/SE\n",
    "quantile = .95\n",
    "z = st.norm.ppf((1+quantile)/2)\n",
    "\n",
    "Z>z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we reject the null hypothesis.\n",
    "\n",
    "Two-sided / one sided:\n",
    "\n",
    "<img src=\"pictures/CI.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/errortypes.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p-value vs z-test\n",
    "\n",
    "𝑝-value indicates how unlikely the statistic is. \n",
    "\n",
    "𝑧-score indicates how far away from the mean it is. \n",
    "\n",
    "There may be a difference between them, depending on the sample size:\n",
    "- for large samples, even small deviations from the mean become unlikely. I.e. the 𝑝-value may be very small even for a low 𝑧-score. \n",
    "- conversely, for small samples even large deviations are not unlikely. I.e. a large 𝑧-score will not necessarily mean a small 𝑝-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-test vs z-test\n",
    "\n",
    "Like z-tests, t-tests are calculations used to test a hypothesis, but they are most useful when we need to determine if there is a statistically significant difference between two independent sample groups. In other words, a t-test asks whether a difference between the means of two groups is unlikely to have occurred because of random chance. Usually, t-tests are most appropriate when dealing with problems with a limited sample size (n < 30).\n",
    "\n",
    "$\\displaystyle t = \\frac {\\left(x_1 — x_2\\right)} {\\left(\\frac{\\sigma} {\\sqrt{n_1}} + \\frac{\\sigma} {\\sqrt{n_2}}\\right)}$\n",
    "\n",
    "Where:\n",
    "- $x_1$ = mean of sample 1\n",
    "- $x_2$ = mean of sample 2\n",
    "- $n_1$ = size of sample 1\n",
    "- $n_2$ = size of sample 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Comparison\n",
    "\n",
    "𝑧-test. A 𝑧-test assumes that our observations are independently drawn from a Normal distribution with unknown mean and known variance. A 𝑧-test is used primarily when we have quantitative data. (i.e. weights of rodents, ages of individuals, systolic blood pressure, etc.) However, 𝑧-tests can also be used when interested in proportions. (i.e. the proportion of people who get at least eight hours of sleep, etc.)\n",
    "\n",
    "𝑡-test. A 𝑡-test assumes that our observations are independently drawn from a Normal distribution with unknown mean and unknown variance. Note that with a 𝑡-test, we do not know the population variance. This is far more common than knowing the population variance, so a 𝑡-test is generally more appropriate than a 𝑧-test, but practically there will be little difference between the two if sample sizes are large.\n",
    "\n",
    "With 𝑧- and 𝑡-tests, your alternative hypothesis will be that your population mean (or population proportion) of one group is either not equal, less than, or greater than the population mean (or proportion) or the other group. This will depend on the type of analysis you seek to do, but your null and alternative hypotheses directly compare the means/proportions from the two groups.\n",
    "\n",
    "Chi-squared test. Whereas 𝑧- and 𝑡-tests concern quantitative data (or proportions in the case of 𝑧), chi-squared tests are appropriate for qualitative data. Again, the assumption is that observations are independent of one another. In this case, you aren't seeking a particular relationship. Your null hypothesis is that no relationship exists between variable one and variable two. Your alternative hypothesis is that a relationship does exist. This doesn't give you specifics as to how this relationship exists (i.e. In which direction does the relationship go) but it will provide evidence that a relationship does (or does not) exist between your independent variable and your groups.\n",
    "\n",
    "Fisher's exact test. One drawback to the chi-squared test is that it is asymptotic. This means that the 𝑝-value is accurate for very large sample sizes. However, if your sample sizes are small, then the 𝑝-value may not be quite accurate. As such, Fisher's exact test allows you to exactly calculate the 𝑝-value of your data and not rely on approximations that will be poor if your sample sizes are small.\n",
    "\n",
    "<img src=\"pictures/hyptest.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA\n",
    "\n",
    "<img src=\"pictures/ftest.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Kmeans / dendrogram / elbow method\n",
    "\n",
    "- Assignment step: Assign each observation to the cluster whose mean has the least squared Euclidean distance, this is intuitively the \"nearest\" mean\n",
    "- Update step: Calculate the new means (centroids) of the observations in the new clusters.\n",
    "\n",
    "The Forgy method randomly chooses k observations from the dataset and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster's randomly assigned points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## PCA\n",
    "\n",
    "### Definition\n",
    "\n",
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. \n",
    "\n",
    "This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. \n",
    "\n",
    "The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.\n",
    "\n",
    "1. The PCs are essentially the linear combinations of the original variables.\n",
    "2. The PCs are orthogonal.\n",
    "3. The least important PCs are also sometimes useful in regression, outlier detection, etc.\n",
    "\n",
    "### Calculation\n",
    "\n",
    "1. Normalize the data\n",
    "\n",
    "2. Calculate the covariance matrix\n",
    "\n",
    "3. Calculate the eigenvalues and eigenvectors of the covariance matrix\n",
    "\n",
    "4. Choosing components and forming a feature vector:\n",
    "\n",
    "We order the eigenvalues from largest to smallest so that it gives us the components in order or significance. Here comes the dimensionality reduction part. If we have a dataset with n variables, then we have the corresponding n eigenvalues and eigenvectors. It turns out that the eigenvector corresponding to the highest eigenvalue is the principal component of the dataset and it is our call as to how many eigenvalues we choose to proceed our analysis with. To reduce the dimensions, we choose the first p eigenvalues and ignore the rest. We do lose out some information in the process, but if the eigenvalues are small, we do not lose much.\n",
    "\n",
    "Next we form a feature vector which is a matrix of vectors, in our case, the eigenvectors. In fact, only those eigenvectors which we want to proceed with. Since we just have 2 dimensions in the running example, we can either choose the one corresponding to the greater eigenvalue or simply take both.\n",
    "\n",
    "$Feature Vector = (eig1, eig2)$\n",
    "\n",
    "5. Forming Principal Components:\n",
    "\n",
    "We take the transpose of the feature vector and left-multiply it with the transpose of scaled version of original dataset.\n",
    "\n",
    "$NewData = FeatureVector^T x ScaledData^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load Digits Data And Make Sparse\n",
    "digits = datasets.load_digits()\n",
    "# Standardize the feature matrix\n",
    "X = StandardScaler().fit_transform(digits.data)\n",
    "# Create a PCA with 2 components\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "# Conduct PCA\n",
    "X_pca = pca.fit_transform(X)\n",
    "print()\n",
    "# Show results\n",
    "print('Original number of features:', X.shape[1])\n",
    "print('Reduced number of features:', X_pca.shape[1])\n",
    "print('Variance retained:', pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('rainbow', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca = PCA().fit(digits.data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/biasvar.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "- Mean Absolute Error is the average of the difference between the Original Values and the Predicted Values. It gives us the measure of how far the predictions were from the actual output. However, they don’t gives us any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data. Mathematically, it is represented as :\n",
    "\n",
    "$\\displaystyle \\operatorname {MAE} ={\\frac {1}{n}}\\sum _{i=1}^{n}|Y_{i}-{\\hat {Y_{i}}}|$\n",
    "\n",
    "- Mean Squared Error(MSE): is quite similar to Mean Absolute Error, the only difference being that MSE takes the average of the square of the difference between the original values and the predicted values. The advantage of MSE being that it is easier to compute the gradient, whereas Mean Absolute Error requires complicated linear programming tools to compute the gradient. As, we take square of the error, the effect of larger errors become more pronounced then smaller error, hence the model can now focus more on the larger errors.\n",
    "\n",
    "$\\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}$\n",
    "\n",
    "- accuracy: It is the ratio of number of correct predictions to the total number of input samples.\n",
    "\n",
    "$\\displaystyle accuracy = \\frac{TP+FP}{TOT}$\n",
    "\n",
    "- Precision : It is the number of correct positive results divided by the number of positive results predicted by the classifier.\n",
    "\n",
    "$\\displaystyle precision = \\frac{TP}{TP+FP}$\n",
    "\n",
    "- Recall or Sensitivity or TPR: It is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). e.g. email spam\n",
    "\n",
    "$\\displaystyle recall = \\frac{TP}{TP+FN}$\n",
    "\n",
    "- False Positive Rate: False Positive Rate corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points. e.g. cancer\n",
    "\n",
    "$\\displaystyle FPR = \\frac{FP}{TN+FP} = 1-Specificity$\n",
    "\n",
    "- F1 Score is the Harmonic Mean between precision and recall. The range for F1 Score is [0, 1]. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).\n",
    "High precision but lower recall, gives you an extremely accurate, but it then misses a large number of instances that are difficult to classify. The greater the F1 Score, the better is the performance of our model. Mathematically, it can be expressed as :\n",
    "\n",
    "$\\displaystyle F1 = \\frac{2}{\\frac{1}{precision}+\\frac{1}{recall}}$\n",
    "\n",
    "- Area Under Curve(AUC) is one of the most widely used metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example.\n",
    "\n",
    "<img src=\"pictures/roc.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "\n",
    "- Logarithmic Loss or Log Loss, works by penalising the false classifications. Accuracy is the count of predictions where your predicted value equals the actual value. Accuracy is not always a good indicator because of its yes or no nature. Log Loss takes into account the uncertainty of your prediction based on how much it varies from the actual label. This gives us a more nuanced view into the performance of our model. Log-loss values [0,+inf[, objective 0.\n",
    "\n",
    "\n",
    "<h1><center>$-{(y\\log(p) + (1 - y)\\log(1 - p))}$</center></h1>\n",
    "\n",
    "Example:\n",
    "- label = 1 and predicted probability of .25: $-{\\log(.25)}$\n",
    "- label = 0 and predicted probability of .25: $-{\\log(0.75)}$\n",
    "\n",
    "<img src=\"pictures/logloss1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"pictures/logloss2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bagging / boosting / stacking\n",
    "\n",
    "- Bootstrapping: Let’s begin by defining bootstrapping. This statistical technique consists in generating samples of size B (called bootstrap samples) from an initial dataset of size N by randomly drawing with replacement B observations.\n",
    "\n",
    "<img src=\"pictures/bootstrap.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "- Bagging: First, we create multiple bootstrap samples so that each new bootstrap sample will act as another (almost) independent dataset drawn from true distribution. Then, we can fit a weak learner for each of these samples and finally aggregate them such that we kind of “average” their outputs and, so, obtain an ensemble model with less variance that its components. Roughly speaking, as the bootstrap samples are approximatively independent and identically distributed (i.i.d.), so are the learned base models. Then, “averaging” weak learners outputs do not change the expected answer but reduce its variance (just like averaging i.i.d. random variables preserve expected value but reduce variance).\n",
    "\n",
    "\n",
    "- Boosting: each model in the sequence is fitted giving more importance to observations in the dataset that were badly handled by the previous models in the sequence. Intuitively, each new model focus its efforts on the most difficult observations to fit up to now, so that we obtain, at the end of the process, a strong learner with lower bias (even if we can notice that boosting can also have the effect of reducing variance). The final result is the weigthed (how well they performed) average of the learners.\n",
    "\n",
    "<img src=\"pictures/bagboo1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"pictures/bagboo2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "- Stacking: the predict proba outputs of every models becomes the feature of a new dataset on which you apply a new model (Logistic Regression generally or heuristic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Gradient\n",
    "\n",
    "<img src=\"pictures/learningrate.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Help gradient descent:\n",
    "- feature scale\n",
    "- mean normalization: $x = x - \\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "TODO:\n",
    "- bayesian method\n",
    "\n",
    "\n",
    "- support vector machine\n",
    "- decision tree\n",
    "- Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "\n",
    "- In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "<img src=\"pictures/knn_c.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "- In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\n",
    "\n",
    "\n",
    "How is k-NN different from k-means clustering?\n",
    "\n",
    "k-NN, or k-nearest neighbors is a classification algorithm, where the k is an integer describing the number of neighboring data points that influence the classification of a given observation. K-means is a clustering algorithm, where the k is an integer describing the number of clusters to be created from the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "- $n$: number of features\n",
    "- $x^i$: ith example\n",
    "- $x_j^i$: jth feature of the ith example\n",
    "\n",
    "<h1><center>$\\large h_\\theta(x)=\\theta^T x$</center></h1>\n",
    "\n",
    "where: \n",
    "- $\\theta \\in {\\rm I\\!R}^{n+1}$\n",
    "- $x \\in {\\rm I\\!R}^{n+1}$ with $x_0^i=0$\n",
    "\n",
    "<h1><center>$ \\displaystyle J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (y^i-h_\\theta(x^i))^2$</center></h1>\n",
    "\n",
    "Repeat until converge: \n",
    "<h1><center>$ \\displaystyle \\theta_j = \\theta_j -\\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
    "    = \\theta_j -\\frac{\\alpha}{m} \\sum_{i=1}^{m} (y^i-h_\\theta(x^i))x^i_j\n",
    "$</center></h1>\n",
    "\n",
    "- analytical solution: normal equation\n",
    "- polynomial regression: add power of x as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "<h1><center>$\\displaystyle \\sigma (t)={\\frac {1}{1+e^{-t}}}$</center></h1>\n",
    "\n",
    "<h1><center>$\\displaystyle h_{\\theta }(X)={\\frac {1}{1+e^{-\\theta ^{T}X}}}=\\Pr(Y=1\\mid X;\\theta )$</center></h1>\n",
    "\n",
    "<h1><center>$ \\displaystyle l(\\theta) = \\sum_{i=1}^{m} y^i \\log(h_\\theta(x^i)) + (1-y^i) \\log(1-h_\\theta(x^i)) $</center></h1>\n",
    "\n",
    "Repeat ($l(\\theta)$ to maximize): \n",
    "\n",
    "<h1><center>$ \\displaystyle \\theta_j = \\theta_j + \\alpha \\frac{\\partial l(\\theta)}{\\partial \\theta_j}\n",
    "    = \\theta_j + \\alpha (y^i-h_\\theta(x^i))x^i_j\n",
    "$</center></h1>\n",
    "\n",
    "Regularisation:\n",
    "- Lasso : L1 $ \\displaystyle \\Longrightarrow l(\\theta) + \\lambda \\sum_{j=1}^{m} \\left| \\theta_j \\right|$\n",
    "- Ridge : L2 $ \\displaystyle \\Longrightarrow l(\\theta) + \\lambda \\sum_{j=1}^{m} \\theta_j^2$\n",
    "- Elastic Net: combinaison of both\n",
    "\n",
    "<img src=\"pictures/ridgelasso.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    " In the ridge, the coefficients of the linear transformation are normal distributed and in the lasso they are Laplace distributed. In the lasso, this makes it easier for the coefficients to be zero and therefore easier to eliminate some of your input variable as not contributing to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Implementation\n",
    "\n",
    "#### 0. intialisation :\n",
    "\n",
    "W -> matrix random but small\n",
    "\n",
    "b -> vector zeros\n",
    "\n",
    "#### 1. forward propagation :\n",
    "\n",
    "⁃ linaire: $Z[l] = W[l] * A[l-1] + b[l]$ with $A[0] = X$\n",
    "\n",
    "⁃ activation function g: $A[l] = g( Z[l] )$\n",
    "\n",
    "- output: $A[l]$ and cache Z[l]\n",
    "\n",
    "<img src=\"pictures/forwardprop.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "#### 2. compute cost :\n",
    "\n",
    "several cost function possible but beware of convexity !!\n",
    "\n",
    "<h1><center>$ \\displaystyle - \\frac{1}{m} (Y \\log(A[L]).T + (1-Y) \\log( 1 - A[L] ).T ) $</center></h1>\n",
    "\n",
    "With A[L] output layer\n",
    "\n",
    "#### 3. backward propagation :\n",
    "\n",
    "- Activation backward : $dZ[l] = dA[l] * g_l’( Z[l] )$\n",
    "- Linear backward : calcul $dW[l]$, $db[l]$ $\\Rightarrow{A[l-1]}$\n",
    "\n",
    "<img src=\"pictures/backwardprop.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "#### 4. update_param :\n",
    "parameters -= alpha*grad with parameters = W,b\n",
    "\n",
    "#### model :\n",
    "reiter all steps on n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Parameters vs Hyperparameters\n",
    "\n",
    "Main parameters of the NN is $W$ and $b$.\n",
    "\n",
    "Hyper parameters (parameters that control the algorithm) are like:\n",
    "- Learning rate.\n",
    "- Number of iteration.\n",
    "- Number of hidden layers L.\n",
    "- Number of hidden units n.\n",
    "- Choice of activation functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Activation functions\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"pictures/activationfct.png\" alt=\"Drawing\" style=\"width: 400px;\"/></td>\n",
    "<td> <img src=\"pictures/activationfct2.png\" alt=\"Drawing\" style=\"width: 400px;\"/></td>\n",
    "</tr></table>\n",
    "\n",
    "Activation functions are used to introduce non-linearities in the linear output of the type w * x + b in a neural network.\n",
    "\n",
    "Sigmoid and tanh should not be used as activation function for the hidden layer. This is because of the vanishing gradient problem, i.e., if your input is on a higher side (where sigmoid goes flat) then the gradient will be near zero. This will cause very slow or no learning during backpropagation as weights will be updated with really small values. The best function for hidden layers is thus ReLu, although, the dead neurons may happen if you use ReLU non-linarity, which is called dying ReLU.\n",
    "\n",
    "- for binary classification : relu -> relu -> … -> sigmoid\n",
    "- for multi class classification : relu -> relu -> … -> softmax\n",
    "- for regression : relu -> relu -> … -> linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Regularization\n",
    "\n",
    "- dropout regularization : randomly turns off neurons\n",
    "- vanishing/explode gradient (for deep network) -> xavier initialisation \n",
    "- early stopping: J still decreases but the prediction is worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Model Optimizers\n",
    "\n",
    "The optimizer is a search technique, which is used to update weights in the model.\n",
    "\n",
    "- mini-batch gradient: for big data set, splits the dataset in batch and update weights at the end of the batch within an iteration\n",
    "- learning rate decay\n",
    "- SGD: Stochastic Gradient Descent and how to address problems like getting stuck in a local minima or a saddle point. \n",
    "- RMSprop: Adaptive learning rate optimization method.\n",
    "- Adam: Adaptive Moment Estimation (Adam) that also uses adaptive learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Types of neural networks\n",
    "\n",
    "- Standard NN (Useful for Structured data)\n",
    "- CNN or convolutional neural networks (Useful in computer vision)\n",
    "- RNN or Recurrent neural networks (Useful in Speech recognition or NLP)\n",
    "\n",
    "RNNs (recurrent neural networks) are made up of one node. It is fed data then outputs aresult back into itself, and continues to do this. Breakthroughs like LSTM (long short termmemory) make it smart at remembering things that have happened in the past and findingpatterns across time to make its next guesses make sense.\n",
    "\n",
    "CNNs (convolutional neural networks) essentially have three parts, convolution layers, pooling layers, and fully-connected layers. It usually takes a 2D (sometimes moredimensions) matrix and outputs a result.\n",
    "\n",
    "Convolution starts at the top left and takes a small window with a certain width and heightand performs an operation on that, the operation is usually a matrix multiplication where thematrix to multiply by is decided via gradient descent to get the best final results. It then moves according to a stride parameter and does the same. It does this all the way across the image and outputs a new image. \n",
    "\n",
    "Pooling is similar in the sense that it breaks the image down using small windows; however,the operation it runs on this small window is usually (average, max, or min) to combine thesmall window into a single pixel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### NLP\n",
    "\n",
    "- TFIDF: In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf–idf.\n",
    "\n",
    "\n",
    "- Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other.\n",
    "\n",
    "\n",
    "- A word embedding is a learned representation for text where words that have the same meaning have a similar representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. The position of a word in the learned vector space is referred to as its embedding. Two popular examples of methods of learning word embeddings from text include: Word2Vec, GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random forest / GBM (gradient boosting machine)\n",
    "\n",
    "- The random forest approach is a bagging method where deep trees, fitted on bootstrap samples, are combined to produce an output with lower variance. However, random forests also use another trick to make the multiple fitted trees a bit less correlated with each others: when growing each tree, instead of only sampling over the observations in the dataset to generate a bootstrap sample, we also sample over features and keep only a random subset of them to build the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Languages comparison\n",
    "\n",
    "https://www.geeksforgeeks.org/c-vs-java-vs-python/\n",
    "\n",
    "https://www.javatips.net/blog/c-vs-java-vs-python-a-comparison\n",
    "\n",
    "\n",
    "### Interpreted (python) vs compiled languages (C/Java)\n",
    "\n",
    "The difference between an interpreted and a compiled language lies in the result of the process of interpreting or compiling. An interpreter produces a result from a program, while a compiler produces a program written in assembly language. The assembler of architecture then turns the resulting program into binary code. Assembly language varies for each individual computer, depending upon its architecture. Consequently, compiled programs can only run on computers that have the same architecture as the computer on which they were compiled.\n",
    "\n",
    "### Python: pass by assignment\n",
    "\n",
    "Remember that arguments are passed by assignment in Python. Since assignment just creates references to objects, there’s no alias between an argument name in the caller and callee, and so no call-by-reference per se. You can achieve the desired effect in a number of ways.\n",
    "\n",
    "If you pass a mutable object into a method, the method gets a reference to that same object and you can mutate it to your heart's delight, but if you rebind the reference in the method, the outer scope will know nothing about it, and after you're done, the outer reference will still point at the original object.\n",
    "\n",
    "If you pass an immutable object to a method, you still can't rebind the outer reference, and you can't even mutate the object.\n",
    "\n",
    "### Python: id() \n",
    "\n",
    "This identity has to be unique and constant for this object during the lifetime. Two objects with non-overlapping lifetimes may have the same id() value. If we relate this to C, then they are actually the memory address, here in Python it is the unique id. This function is generally used internally in Python.\n",
    "\n",
    "### Python: memory\n",
    "\n",
    "In Python, memory is managed in a private heap space. This means that all the objects and data structures will be located in a private heap. However, the programmer won’t be allowed to access this heap. Instead, the Python interpreter will handle it. At the same time, the core API will enable access to some Python tools for the programmer to start coding. The memory manager will allocate the heap space for the Python objects while the inbuilt garbage collector will recycle all the memory that’s not being used to boost available heap space. \n",
    "\n",
    "## Python: Mutable vs immutable\n",
    "\n",
    "Common immutable type:\n",
    "- numbers: int(), float(), complex()\n",
    "- immutable sequences: str(), tuple(), frozenset(), bytes()\n",
    "\n",
    "Common mutable type (almost everything else):\n",
    "- mutable sequences: list(), bytearray()\n",
    "- set type: set()\n",
    "- mapping type: dict()\n",
    "- classes, class instances\n",
    "All immutable built-in objects in python are hashable. Mutable containers like lists and dictionaries are not hashable while immutable container tuple is hashable\n",
    "\n",
    "Tuples are smaller. Tuples have structure, lists have order, set are list without order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL\n",
    "\n",
    "\n",
    "### What is a primary key?\n",
    "\n",
    "A primary key is a combination of fields which uniquely specify a row. This is a special kind of unique key, and it has implicit NOT NULL constraint. It means, Primary key values cannot be NULL.\n",
    "\n",
    "### What is a unique key?\n",
    "\n",
    "A Unique key constraint uniquely identified each record in the database. This provides uniqueness for the column or set of columns.\n",
    "\n",
    "A Primary key constraint has automatic unique constraint defined on it. But not, in the case of Unique Key.\n",
    "\n",
    "There can be many unique constraint defined per table, but only one Primary key constraint defined per table.\n",
    "\n",
    "### What is a foreign key?\n",
    "\n",
    "A foreign key is one table which can be related to the primary key of another table. Relationship needs to be created between two tables by referencing foreign key with the primary key of another table.\n",
    "e.g Order table with foreign key person_id from table person\n",
    "\n",
    "### What is a constraint?\n",
    "\n",
    "Constraint can be used to specify the limit on the data type of table. Constraint can be specified while creating or altering the table statement. Sample of constraint are.\n",
    "\n",
    "- NOT NULL.\n",
    "- CHECK.\n",
    "- DEFAULT.\n",
    "- UNIQUE.\n",
    "- PRIMARY KEY.\n",
    "- FOREIGN KEY.\n",
    "\n",
    "### Which operator is used in query for pattern matching?\n",
    "\n",
    "LIKE operator is used for pattern matching, and it can be used as -.\n",
    "\n",
    "- % - Matches zero or more characters.\n",
    "- _ – Matching exactly one character.\n",
    "\n",
    "\n",
    "`Select * from Student where studentname like 'a%'`\n",
    "\n",
    "`Select * from Student where studentname like ‘ami_'`\n",
    "\n",
    "### What is Union, minus and Intersect commands?\n",
    "\n",
    "UNION operator is used to combine the results of two tables, and it eliminates duplicate rows from the tables.\n",
    "\n",
    "UNION ALL does not (eliminate duplicates).\n",
    "\n",
    "MINUS/EXEPT operator is used to return rows from the first query but not from the second query. \n",
    "\n",
    "INTERSECT operator is used to return rows returned by both the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
